{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wXip_npz09_"
      },
      "source": [
        "## CLASIFICACIÓN DE IMÁGENES MEDIANTE REDES NEURONALES CONVOLUCIONALES\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfCMCxAZTXtZ"
      },
      "source": [
        "### Presentación\n",
        "\n",
        "En los últimos años las redes neuronales o *Neural Networks* (NN) han demostrado su capacidad para resolver problemas que eran complejos e, incluso, imposibles de solucionar mediante técnicas tradicionales.\n",
        "\n",
        "Uno de estos problemas es la clasificación de imágenes, que forma parte del ámbito de la visión por computador. Más especificamente, la clasificación de imágenes pretende agrupar imágenes dependiendo de lo que está representado en ellas. En otras palabras, la clasificación de imágenes tiene por objetivo asignar una clase a cada imagen analizada. Este es uno de los ámbitos en los que las redes neuronales convolucionales o *Convolutional Neural Networks* (CNN) destacan.\n",
        "\n",
        "En esta práctica exploraremos el uso de CNN en clasificación de imágenes analizando distintas arquitecturas, distintas formas de entrenarlas y descubriendo las ventajas del denominado *transfer learning*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6UhmuehTb8G"
      },
      "source": [
        "### Competencias\n",
        "\n",
        "En este enunciado se trabajan las siguientes competencias generales:\n",
        "- Capacidad para proyectar, calcular y diseñar productos, procesos e\n",
        "instalaciones en todos los ámbitos de la ingeniería informática.\n",
        "- Capacidad para el modelado matemático, cálculo y simulación en\n",
        "centros tecnológicos y de ingeniería de empresa, particularmente en\n",
        "tareas de investigación, desarrollo e innovación en todos los ámbitos\n",
        "relacionados con la ingeniería informática.\n",
        "- Capacidad para aplicar los conocimientos adquiridos y solucionar\n",
        "problemas en entornos nuevos o poco conocidos dentro de contextos\n",
        "más amplios y multidisciplinares, siendo capaces de integrar estos\n",
        "conocimientos.\n",
        "- Poseer habilidades para el aprendizaje continuo, autodirigido y\n",
        "autónomo.\n",
        "- Capacidad para modelar, diseñar, definir la arquitectura, implantar,\n",
        "gestionar, operar, administrar y mantener aplicaciones, redes,\n",
        "sistemas, servicios y contenidos informáticos.\n",
        "\n",
        "Las competencias específicas de esta asignatura que se trabajan en esta\n",
        "prueba son:\n",
        "- Entender qué es el aprendizaje automático en el contexto de la\n",
        "inteligencia artificial.\n",
        "- Distinguir entre los diferentes tipos y métodos de aprendizaje.\n",
        "- Aplicar las técnicas estudiadas a un caso real."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuNVYxUITovf"
      },
      "source": [
        "### Objetivos\n",
        "\n",
        "En esta práctica aprenderéis:\n",
        "\n",
        "* Qué es el *overfitting* o sobreentrenamiento y como reducirlo mediante aumentación de datos o *data augmentation*.\n",
        "* Cómo construir, entrenar y evaluar una CNN enfocada a clasificación de imágenes.\n",
        "* Cómo aprovechar el *transfer learning*.\n",
        "* Cómo evaluar y comparar distintas arquitecturas de red."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MVVhDnHT_-Q"
      },
      "source": [
        "### Recursos\n",
        "\n",
        "Esta práctica requiere los recursos siguientes:\n",
        "\n",
        "Archivos proporcionados:\n",
        "\n",
        "  * Este fichero .ipynb que estáis leyendo ahora mismo.\n",
        "  * El fichero DATA.ZIP que contiene las imágenes con las que trabajaréis.\n",
        "\n",
        "Complementarios:\n",
        "  * Manual de teoría de la asignatura.\n",
        "  * Documentación de las bibliotecas utilizadas. En esta práctica trabajaréis especialmente con Keras y Tensorflow, aunque también utilizaréis bibliotecas como NumPy y skimage. Es recomendable, por lo tanto, leer la documentación de dichas bibliotecas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pm4AkmzXUWPx"
      },
      "source": [
        "### Entrega y criterios de evaluación\n",
        "\n",
        "La práctica se debe entregar el **1 de marzo del 2024**.\n",
        "\n",
        "La entrega debe incluir una versión editada de este cuaderno (.ipynb). Se recomienda el uso de Google Colab (https://colab.research.google.com/). El código de las soluciones a los ejercicios se debe implementar y ejecutar en las celdas de código proporcionadas y la respuestas justificadas se deben agregar a las celdas de texto correspondientes.\n",
        "\n",
        "Todas las respuestas deben estar correctamente razonadas y justificadas. **Las soluciones que no vayan acompañadas de la correspondiente respuesta razonada no serán evaluadas**.\n",
        "\n",
        "Los ejercicios se valoraran de la siguiente forma:\n",
        "* Ejercicio 1: 2 puntos\n",
        "* Ejercicio 2: 1.5 puntos\n",
        "* Ejercicio 3: 3 puntos\n",
        "* Ejercicio 4: 1 punto\n",
        "* Ejercicio 5: 1.5 puntos\n",
        "* Ejercicio 6: 1 punto\n",
        "\n",
        "Cada ejercicio será evaluado teniendo en cuenta tanto la corrección técnica de la solución como la justificación y argumentación del procedimiento y los resultados.\n",
        "\n",
        "Cada ejercicio está dividido en distintos apartados (a,b,c,...). Esta división tiene por objetivo guiaros en la resolución de los ejercicios y cada apartado debe ser solucionado, proporcionando el código y las explicaciones requeridas, pero *no* se evalúan individualmente. Cada ejercicio se evaluará globalmente, por lo que no hay puntuación específica para cada apartado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxm-7MuJU6bK"
      },
      "source": [
        "### Descripción de la PEC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftAubnZZ8Dii"
      },
      "source": [
        "#### Los datos\n",
        "\n",
        "Una característica distintiva de las redes neuronales es la necesidad de grandes cantidades de datos para entrenarlas. Por este motivo, en esta práctica trabajaréis con el *Oxford III Pet Dataset*, el cual contiene más de 7000 imágenes etiquetadas de perros y gatos. Los datos (*dataset*) originales están disponibles en el siguiente enlace:\n",
        "\n",
        "https://www.robots.ox.ac.uk/~vgg/data/pets/\n",
        "\n",
        "Desafortunadamente este dataset contiene varios errores y, además, combina imágenes con disintas resoluciones, dificultando su uso. Por este motivo hemos preparado una versión adaptada del dataset que os proporcionamos en el fichero DATA.ZIP.\n",
        "\n",
        "Por lo tanto, *no* debéis trabajar con el dataset original sino con la versión adaptada que os proporciono. Esta versión está compuesta por 4780 imágenes en color (RGB) de perros y gatos en formato JPG a una resolución de 128x128 píxeles. Las imágenes son los datos que queremos que nuestra red neuronal aprenda a clasificar como *imagen de perro* o *imagen de gato*.\n",
        "\n",
        "Los nombres de los ficheros carecen de significado salvo por su primera letra. Si la primera letra es una \"C\" la imagen muestra un gato (*cat*). Si la primera letra es una \"D\" la imagen es de un perro (*dog*). Esto es extremadamente importante, dado que significa que el *ground truth* se proporciona en el propio nombre del archivo. Aseguráos de tener claro este aspecto abriendo algunas imágenes cuyo nombre de archivo empiece por C o D y comprobando que las imágenes son de gatos (C) y perros (D) respectivamente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1Cqelcc8Dii"
      },
      "source": [
        "#### El entorno de trabajo\n",
        "\n",
        "Como ya se indicó, esta práctica se entregará como un fichero ipynb. Podéis trabajar con dicho fichero lovalmente mediante la plataforma Jupyter (Jupyter Lab o Jupyter Notebook). En este caso deberéis instalar Keras y Tensorflow además de bibliotecas como scikit-image y scikit-learn entre otras.\n",
        "\n",
        "Para evitar problemas de dependencias entre bibliotecas, de versiones o de conflictos entre distintas instalaciones en vuestro ordenador, recomendamos trabajar con la plataforma Google Colab:\n",
        "\n",
        "https://colab.research.google.com/notebooks/welcome.ipynb\n",
        "\n",
        "Una vez accedáis al link anterior, deberéis identificaros con las credenciales de Google. Podéis utilizar (y, de hecho, os recomendamos que lo hagáis) vuestras credenciales de Google de la UOC.\n",
        "\n",
        "En Colab podréis trabajar con el ipynb que os proporcionamos y encontraréis Keras, Tensorflow, sklearn y otras bibliotecas ya instaladas. En caso de necesitar alguna biblioteca no instalada, podéis instalarla desde Colab con el comando:\n",
        "```\n",
        "!pip install biblioteca-a-instalar\n",
        "```\n",
        "\n",
        "Antes de empezar a trabajar en esta práctica es muy recomendable que os familiaricéis con Google Colab. En particular, es muy importante que tengáis en cuenta que *todos* los ficheros que generéis o guardéis en el espacio de Google Colab se pierden al finalizar o abandonar la sesión. Por lo tanto, deberéis decidir como hacer estos datos persistentes si necesitáis que lo sean: podéis descargarlos a vuestro disco duro local antes de abandonar la sesión y volverlos subir al volver a empezar o también podéis vincular vuestro Google Drive a Colab para poder acceder a él directamente.\n",
        "\n",
        "Es importante que tengáis en cuenta que los accesos entre Colab y Google Drive son *muy* lentos. Por lo tanto, no es recomendable que vuestros programas accedan a Google Drive directamente. Es mejor copiar los datos de Drive a Colab al principio de una sesión y de Colab a Drive al final.\n",
        "\n",
        "Podéis encontrar información relativa a estos aspectos en el siguiente enlace:\n",
        "\n",
        "https://colab.research.google.com/notebooks/io.ipynb\n",
        "\n",
        "En cualquier caso, la forma en la que gestionéis los ficheros es decisión vuestra y forma parte de la práctica. Los profesores no os proporcionaremos código para gestionar estos aspectos. Además, en el aula se dará soporte a Google Colab pero no a otros entornos.\n",
        "\n",
        "Una herramienta útil para desarrollar, entrenar y evaluar una red neuronal es TensorBoard. No hay ninguna pregunta relacionada con TensorBoard en esta práctica, aunque podéis utilizarlo si lo consideráis útil. No obstante, con independencia de que utilicéis o no TensorBoard, debéis resolver y responder a los ejercicios de la forma exacta en la que se solicita. Encontraréis información detallada sobre TensorBoard aquí:\n",
        "\n",
        "https://www.tensorflow.org/tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSvdg7408Dii"
      },
      "source": [
        "#### Tiempo de entrenamiento\n",
        "\n",
        "Tened en cuenta que entrenar una red neuronal consume mucho tiempo. Si trabajáis localmente podéis reducir el tiempo de entrenamiento usando una GPU. Desgraciadamente ni todas las GPU se pueden utilizar en este contexto ni instalar sus controladores es fácil. Los profesores *no* proporcionaremos soporte para la instalación de controladores de GPU, CUDA, etcétera por un motivo fundamental: es una tarea más próxima a la magia (o a su equivalente informático, el copy/paste de Stack Overflow) que a la ingeniería. Es por este motivo que *no* recomendamos esta opción a no ser que ya tengáis una GPU adecuadamente instalada y funcionando con Keras+Tensorflow en Python en vuestro ordenador.\n",
        "\n",
        "Si utilizáis Colab podéis utilizar las GPU de Google, de forma que el tiempo de entrenamiento se reduzca significativamente respecto a un entrenamiento sobre CPU. No obstante, no esperéis milagros. Si bien \"GPU de Google\" suena a algo muy rápido, no lo es tanto como pueda parecer.\n",
        "\n",
        "Por todos estos motivos, los ejercicios propuestos se centran en redes neuronales simples y entrenamientos pequeños (tan solo unas pocas épocas o *epochs*). Estos entrenamientos pequeños sobre redes neuronales simples no os permitirán ver todo el potencial del *deep learning* pero sí os darán pistas sobre como funcionan las redes neuronales así como de sus ventajas e inconvenientes. Si, tras finalizar la práctica, queréis repetir todos los entrenamientos con más épocas (entre 100 y 200 por ejemplo) o intentar ampliar los modelos sencillos con los que habéis trabajado podéis hacerlo (sin que forme parte de la práctica a entregar).\n",
        "\n",
        "Algunos ejercicios requieren funciones, variables o modelos creados y entrenados en ejercicios anteriores. Si abandonáis la sesión (tanto en local como en remoto) para continuar más adelante, tendréis que volver a ejecutar todas las celdas. Esto no es problemático salvo por los entrenamientos (dado que tardan tiempo en completarse). Dado que se os pedirá que guardéis modelos entrenados en disco, nuestro consejo es que comprobéis (en vuestro código) si el modelo entrenado ya se ha guardado y, de ser así, lo carguéis en lugar de volver a entrenar. Podéis hacer esto incluso si el ejercicio no lo pide expresamente.\n",
        "\n",
        "En relación a lo anterior, recordad que los ficheros en el espacio de Colab se pierden al cerrar la sesión. Esto incluye los modelos entrenados que podáis haber guardado, por lo que para preservarlos deberéis utilizar las técnicas comentadas anteriormente.\n",
        "\n",
        "Como referencia, si trabajáis en local con una CPU *normal*, podéis esperar tiempos de entrenamiento en los modelos de esta práctica de entre 1 y 2 minutos por *epoch*. Si disponéis de una GPU *normal* y la podéis utilizar para el entrenamiento, los tiempos se reducirán a entre 5 y 15 segundos por *epoch*. En Google Colab con GPU podéis esperar tiempos de entrenamiento de entre 10 y 15 segundos por *epoch*, aunque este tiempo depende mucho de la disponibilidad de los servidores de Google."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbTsYgHQ8Dij"
      },
      "source": [
        "#### Tamaño de los modelos\n",
        "\n",
        "Los modelos que guardaréis en disco en esta práctica tienen tamaños que oscilan desde unas decenas de MB (los modelos simples) hasta más de 100MB (el modelo basado en Xception). Tened estos tamaños en consideración para no saturar vuestro disco duro o vuestra cuenta de Google."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6tIDwGA8Dij"
      },
      "source": [
        "#### Sobre Keras\n",
        "\n",
        "Keras ha sufrido algunos cambios recientemente que afectan a la forma de importar sus componentes. Si, en algún momento, intentáis importar un componente de Keras con **import keras.xxx** y os aparece un mensaje de error simplemente debéis cambiarlo a **import tensorflow.keras.xxx** y viceversa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGkmLXEYU-bp"
      },
      "source": [
        "### Ejercicios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Q4GSmx08Dij"
      },
      "source": [
        "#### **Ejercicio 1: Entendiendo el conjunto de datos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gMlA-Ve8Dij"
      },
      "source": [
        "**1-a) Cread dos listas denominadas catFiles y dogFiles. La primera debe contener los nombres de los archivos (sólo el nombre, sin la ruta) de las imágenes que contengan gatos y la segunda los nombres de archivo de las imágenes que contengan perros. Para diferenciar gatos y perros utilizad la primera letra del nombre del archivo (C para gatos y D para perros). Después, imprimid el texto 'CATS: x, DOGS: y' donde x e y son el número de imágenes de gatos y perros respectivamente.**\n",
        "\n",
        "**Consejo: No es necesario leer las imágenes en este ejercicio, tan solo necesitáis acceder a los nombres de los archivos. Para ello podéis utilizar el paquete \"os\" (import os).**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEcrMYXo8Dij"
      },
      "outputs": [],
      "source": [
        "# WRITE YOUR CODE IN THIS CELL AND EXECUTE IT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eV4j-UMQ8Dik"
      },
      "source": [
        "**1-b) Cread la función plot\\_pets(imgPath,fileNames,numImages) encargada de dibujar las primeras *numImages* en *fileNames* una al lado de otra (es decir, una fila de *numImages* imágenes). El parámetro de entrada *imgPath* representa la ruta a las imágenes (por ejemplo, 'DATA'). Sobre cada imagen mostrad el texto \"NROWSxNCOLSxNCHANS,TYPE\" donde NROWS es el número de filas (altura) de la imagen correspondiente, NCOLS es el número de columnas (anchura), NCHANS es el número de canales de color y TYPE es el tipo de datos de la matriz imagen.**\n",
        "\n",
        "**Después, utilizad plot\\_pets para dibujar los primeros cinco gratos en catFiles y los primeros cinco perros en dogFiles.**\n",
        "\n",
        "**Consejo: Tras cargar una imagen, ésta es simplemente un array NumPy donde la primera y la segunda dimensión son las filas y las columnas y la tercera se refiere a los canales de color. En cuanto al tipo, encontraréis información sobre como obtenerlo en la documentación de NumPy. Os recomendamos utilizar la biblioteca skimage para leer las imágenes y matplotlib.pyplot para mostrarlas en pantalla, aunque hay otras opciones posibles.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwW36z9g8Dik"
      },
      "outputs": [],
      "source": [
        "# WRITE YOUR CODE IN THIS CELL AND EXECUTE IT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eJO6spF8Dik"
      },
      "source": [
        "**1-c) En ocasiones el entrenamiento de una red neuronal produce el denominado *overfitting* (sobreentrenamiento). Cuando esto ocurre, la red neuronal empieza a memorizar los datos de entrenamiento en lugar de aprender de ellos. Evitar el sobreentrenamiento es complicado, pero un buen punto de partida es el de disponer de más datos de entrenamiento. Dado que esto no siempre es posible, una técnica habitual consiste en realizar la denominada \"aumentación de datos\" o *data augmentation*.**\n",
        "\n",
        "**La aumentación de datos consiste en crear variaciones de los datos existentes de forma realista. Por ejemplo, se podría hacer un *flip* horizontal de cada imagen (generando así una imagen espejada) y el resultado continuaría siendo realista. Por el contrario, hacer un *flip* vertical no daría como resultado una imagen realista ya que los perros y los gatos no están (habitualmente) con la cabeza abajo y las patas arriba. En este ejercicio programaréis una función capaz de crear tres variaciones distintas (y combinables entre ellas) de una imagen de entrada.**\n",
        "\n",
        "**Cread la función create\\_variation(theImage,doFlip,doNoise,doRotate) que hace lo siguiente:**\n",
        "\n",
        "* **Convierte la imagen a una representación en punto flotante donde cada canal en cada píxel se representa como un valor en punto flotante entre 0 y 1. Esto es importante dado que las redes neuronales se comportan mejor con estos valores. Utilizad la función img\\_as\\_float de skimage para ello.**\n",
        "* **Si doFlip==True, la función voltea horizontalmente la imagen. Buscad en la documentación de NumPy información sobre la función fliplr.**\n",
        "* **Si doNoise==True, la función corrompe ligeramente la image para simular problemas con la cámara o el CCD. Utilizad la función random\\_noise de skimage con los parámetros por defecto.**\n",
        "* **Si doRotate==True, la función rota la image respecto a su centro un ángulo aleatorio entre -45 y +45 grados. Utilizad la función rotate de skimage con el modo 'symmetric' para rellenar los huecos que surgen tras la rotación.**\n",
        "\n",
        "**Notad que la función debe retornar en todos los casos una imagen del mismo tamaño y número de canales que la imagen de entrada, aunque convertida al formato en punto flotante mencionado anteriormente. Notad también que el resultado de la función puede consistir en no aplicar ninguna transformación a la imagen, en aplicar una o en aplicar varias, dependiendo de las combinaciones de valores de doFlip, doNoise y doRotate.**\n",
        "\n",
        "**Tras ello, utilizad la función para dibujar 40 imagenes distribuidas en 8 filas y 5 columnas. Cada columna se corresponde con una imagen del dataset (podéis elegirlas como queráis) y cada fila se corresponde a una de las 8 posibles combinaciones de doFlip, doNoise y doRotate (desde doFlip=doNoise=doRotate=False hasta doFlip=doNoise=doRotate=True).**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XI9eULe-8Dik"
      },
      "outputs": [],
      "source": [
        "# WRITE YOUR CODE IN THIS CELL AND EXECUTE IT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqLGFMRv8Dil"
      },
      "source": [
        "**1-d) Para entrenar una red neuronal es habitual disponer de tres conjuntos de datos: el conjunto de entrenamiento (que se utiliza para entrenar la red), el conjunto de validación (que se utiliza para evaluar el entrenamiento y modificar la red neuronal si es necesario) y el conjunto de test (que se utiliza para la evaluación final de la red). Estos conjuntos deben ser disjuntos y no deben dar preferencia a ninguna clase o distribución de datos.**\n",
        "\n",
        "**En este ejercicio deberéis crear la función split\\_datafiles la cual debe generar estos tres conjuntos de datos a partir del dataset proporcionado. En concreto, la función debe retornar las tres siguientes listas de archivos (sin la ruta, sólo el nombre de los archivos):**\n",
        "\n",
        "* **trainSet: Debe contener el 70% de los nombres de archivo existentes en el dataset.**\n",
        "* **testSet: Debe contener el 20% de los nombres de archivo existentes en el dataset.**\n",
        "* **valSet: Debe contener el 10% de los nombres de archivo existentes en el dataset.**\n",
        "\n",
        "**Estas tres listas deben ser disjuntas (sin nombres repetidos ni dentro de una lista ni entre ellas), deben tener un orden aleatorio (sin ningún orden específico) y las tres listas combinadas deben contener todos los nombres de archivo del conjunto de datos de partida.**\n",
        "\n",
        "**Tras programar la función, ejecutadala y almacenad la salida entre variables: trainSet, testSet y valSet. Recordad que estas variables son listas que contienen nombres de archivo. No contienen las imágenes, tan solo los nombres de archivo.**\n",
        "\n",
        "**Aunque es muy probable que la selección aleatoria genere conjuntos de entrenamiento, validación y test balanceados, no hay garantías de que sea así. Por lo tanto, una vez generados trainSet, testSet y valSet debéis aseguraros (por programa) que el número de gatos y perros en cada uno de ellos no difiera en más de un 10%. Por ejemplo, un conjunto con 100 gatos y 105 perros sería correcto pero un conjunto con 100 gatos y 111 perros se consideraría no balanceado. Aseguraos que los tres conjuntos estan balanceados y, en caso de no ser así, volved a generarlos hasta que lo estén.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84mRsKWl8Dil"
      },
      "outputs": [],
      "source": [
        "# WRITE YOUR CODE IN THIS CELL AND EXECUTE IT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BncACd9k8Dil"
      },
      "source": [
        "#### **Ejercicio 2: Generación de datos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "turznhHn8Dil"
      },
      "source": [
        "Keras proporciona distintos métodos para alimentar una red neuronal durante el entrenamiento, la validación y el test. Estos métodos reciben el nombre genérico de generadores de datos o *data generators*. Por ejemplo, Keras proporciona generadores de datos específicos para imágenes muy completos y totalmente funcionales. En este ejercicio, sin embargo, construiréis una clase *DataGenerator* muy simple en lugar de utilizar las ya existentes, de forma que aprenderéis cómo funcionan los generadores de datos, qué es la generación de datos y como realizar aumentación de datos dentro del propio generador.\n",
        "\n",
        "En este contexto, la clase DataGenerator debe heredar de la clase Sequence y debe proporcionar al menos dos métodos:\n",
        "\n",
        "* \\_\\_len\\_\\_(): Este método retorna el número de lotes (o *batches*) diferentes que puede proporcionar el DataGenerator.\n",
        "* \\_\\_getitem\\_\\_(i): Este método proporciona la imágenes del lote i-ésimo y sus correspondientes clases (su *ground truth*).\n",
        "\n",
        "Aunque no es necesario, es muy recomendable que un generador de datos también proporcione el método on\\_epoch\\_end, el cual se ejecuta tras cada epoch durante en entrenamiento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14qRVuXq8Dil"
      },
      "source": [
        "**2-a) En este ejercicio debéis completar la clase DataGenerator que se os proporciona parcialmente implementada en la siguiente celda. Como podréis observar, sólo debéis completar el método \\_load\\_image\\_, el cual se una internamente por \\_\\_getitem\\_\\_. Deberéis decidir lo que hace \\_load\\_image\\_ a partir de los comentarios proporcionados en el código y de los métodos ya programados.**\n",
        "\n",
        "**Tras completar la clase, ejecutad las siguientes cuatro líneas para crear cuatro objetos DataGenerator: dos DataGenerator de entrenamiento, uno de de test y uno de validación. El primer parámetro de entrada (trainSet, testSet y valSet respectivamente) es la salida de la función split\\_datafiles que habéis creado anteriormente.**\n",
        "\n",
        "```\n",
        "trainGenerator1=DataGenerator(trainSet,False)\n",
        "trainGenerator2=DataGenerator(trainSet,True)\n",
        "testGenerator=DataGenerator(testSet,False)\n",
        "valGenerator=DataGenerator(valSet,False)\n",
        "```\n",
        "\n",
        "**Notad lo siguiente:**\n",
        "\n",
        "* **La aumentación de datos solicitada es casi idéntica a lo que ya habéis hecho en la función create\\_variation. Podéis llamar a esta función o bien programarlo todo directamente dentro de \\_load\\_image\\_.**\n",
        "* **La ejecución de las cuatro líneas mencionadas os puede ayudar a detectar errores en el código del DataGenerator.**\n",
        "* **Dichas cuatro líneas ya están incluidas al final de la siguiente celda. Por lo tanto, no debéis volver a incluirlas.**\n",
        "* **Se crean dos objetos DataGenerator para entrenamiento (trainGenerator1 y trainGenerator2). Uno incluye aumentación de datos (trainGenerator2) y el otro no (trainGenerator1). En los siguientes ejercicios se explorará el uso de ambos.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LB_Bav5H8Dil"
      },
      "outputs": [],
      "source": [
        "# WRITE YOUR CODE IN THIS CELL AND EXECUTE IT.\n",
        "\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from skimage.io import imread\n",
        "from skimage.util import img_as_float,random_noise\n",
        "from skimage.transform import rotate\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "\n",
        "class DataGenerator(Sequence):\n",
        "    # Constructor. Input parameters are:\n",
        "    # * fileNames   : List of sample file names, as provided by split_datafiles. This allows to build\n",
        "    #                 a train data generator, a test data generator or a validation data generator just\n",
        "    #                 by using the corresponding split_datafiles output.\n",
        "    # * doRandomize : If True, the provided file names are shuffled after each training epoch. Also, each\n",
        "    #                 individual image, when loaded, can be left unchanged, flipped, corrupted with\n",
        "    #                 noise or rotated (as in previous activity). Each of the 8 possible combinations\n",
        "    #                 (which included leaving the image unchanged) is chosen randomly with equal probability.\n",
        "    #                 If False, file names are not shuffled and each image is provided unchanged.\n",
        "    # * imgPath     : Path to the images (i.e. DATA)\n",
        "    # * batchSize   : Number of sample images and ground truth items in each batch\n",
        "    def __init__(self,fileNames,doRandomize=False,imgPath='DATA',batchSize=10):\n",
        "        # Store parameters\n",
        "        self.imgPath=imgPath\n",
        "        self.fileNames=fileNames.copy()\n",
        "        self.batchSize=batchSize\n",
        "        self.doRandomize=doRandomize\n",
        "        # Get number of files (to avoid computing them later)\n",
        "        self.numImages=len(self.fileNames)\n",
        "        # Shuffle them if required\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    # Shuffle data at the end of every epoch if required\n",
        "    def on_epoch_end(self):\n",
        "        if self.doRandomize:\n",
        "            random.shuffle(self.fileNames)\n",
        "\n",
        "    # Returns the number ot batches\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(float(self.numImages)/float(self.batchSize)))\n",
        "\n",
        "    # Given an index, loads the image, performs data augmentation if doRandomize\n",
        "    # is True by possibly modifying the image (see comments in the class header)\n",
        "    # and outputs the (possibly transformed) image and its class. In all cases,\n",
        "    # the returned image must be of \"float\" type (use img\\_as\\_float).\n",
        "    # Input  : theIndex - Index of the image to load within self.fileNames.\n",
        "    # Output : theImage - Loaded (and possibly transformed) image. Must be\n",
        "    #                     of float type with values within [0,1]\n",
        "    #          theClass - 0: Cat, 1: Dog\n",
        "    def _load_image_(self,theIndex):\n",
        "\n",
        "# ---> PUT YOUR CODE BETWEEN THIS LINE...\n",
        "\n",
        "\n",
        "# ... AND THIS LINE <---\n",
        "\n",
        "        return theImage,theClass\n",
        "\n",
        "    # Provides the \"theIndex-th\" batch\n",
        "    # Batch format:\n",
        "    # - X : The data. Numpy array of shape (bs,nr,nc,3)\n",
        "    # - y : The ground truth. Numpy array of shape (bs,1)\n",
        "    # Where nb=batch size, nr=num rows, nc=num cols\n",
        "    # Note that in our dataset all images have the same size and number of\n",
        "    # channels. Otherwise, the __getitem__ method should also take care of\n",
        "    # different resolutions and color encodings.\n",
        "    def __getitem__(self,theIndex):\n",
        "        X=[]\n",
        "        y=[]\n",
        "        bStart=max(theIndex*self.batchSize,0)\n",
        "        bEnd=min((theIndex+1)*self.batchSize,self.numImages)\n",
        "        for i in range(bStart,bEnd):\n",
        "            [curImage,curGT]=self._load_image_(i)\n",
        "            X.append(curImage)\n",
        "            y.append(curGT)\n",
        "        return np.array(X),np.array(y)\n",
        "\n",
        "trainGenerator1=DataGenerator(trainSet,False)\n",
        "trainGenerator2=DataGenerator(trainSet,True)\n",
        "testGenerator=DataGenerator(testSet,False)\n",
        "valGenerator=DataGenerator(valSet,False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohqxjQRZ8Dim"
      },
      "source": [
        "**2-b) Cread la función plot\\_batch(X,y) encargada de mostrar las imágenes de un lote (*batch*) una al lado de otra (una fila de imágenes). El parámetro de entrada \"X\" es un lote de imágenes e \"y\" es un lote de clases (*ground truth*) tal y como las proporciona el método \\_\\_getitem\\_\\_ del DataGenerator. Sobre cada imagen la función debe mostrar el texto CAT o DOG dependiendo de la classe correspondiente según el ground truth.**\n",
        "\n",
        "**Después ejecutad las siguientes líneas (ya proporcionadas) para dibujar el primer lote de los dos DataGenerator de entrenamiento:**\n",
        "\n",
        "```\n",
        "[X,y]=trainGenerator1.__getitem__(0)\n",
        "plot_batch(X,y)\n",
        "[X,y]=trainGenerator2.__getitem__(0)\n",
        "plot_batch(X,y)\n",
        "```\n",
        "\n",
        "**Dado que trainGenerator2 realiza aumentación de datos, si ejecutáis la celda varias veces deberíais ver diferentes resultados cada vez en la segunda fila de imágenes.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p10Pgf6Y8Dim"
      },
      "outputs": [],
      "source": [
        "# WRITE YOUR CODE IN THIS CELL AND EXECUTE IT.\n",
        "\n",
        "# ---> PUT YOUR CODE BETWEEN THIS LINE...\n",
        "\n",
        "\n",
        "# ... AND THIS LINE <---\n",
        "\n",
        "# Plot as requested\n",
        "[X,y]=trainGenerator1.__getitem__(0)\n",
        "plot_batch(X,y)\n",
        "[X,y]=trainGenerator2.__getitem__(0)\n",
        "plot_batch(X,y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0eds9J78Dim"
      },
      "source": [
        "#### **Ejercicio 3: Creación de un clasificador sencillo**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHsx9xwJ8Dim"
      },
      "source": [
        "La arquitectura de red neuronal más habitual para clasificar imágenes está compuesta por:\n",
        "\n",
        "* Un **feature extractor** (extractor de características) encargado de transformar la imagen de entrada a una representación más pequeña a través de capas consecutivas (típicamente capas convolucionales y de *pooling*).\n",
        "* Un **clasificador** a cargo de transformar la salida del extractor de características en una clase. Los clasificadores se componen, normalmente, de una capa de *flattening* (aplanamiento) que convierte la salida del clasificador en un tensor 1D; y de un conjunto de capas densas (*fully connected layers*). La última de estas capas (la capa de salida) representará la clase.\n",
        "\n",
        "Buscad información sobre redes neuronales convolucionales (CNN) para clasificación de imágenes. Buscad también información en la documentación de Keras sobre los modelos secuenciales (*Sequential models*) y las capas Conv2D, MaxPooling2D, Flatten y Dense entre otras.\n",
        "\n",
        "Una vez asimilados estos conceptos podréis completar este ejercicio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UE20cowN8Dim"
      },
      "source": [
        "**3-a) Completad el código en la siguiente celda de forma que el modelo resultante cumpal con los siguientes requisitos:**\n",
        "\n",
        "* **La dimensión de los datos de entrada (input\\_shape) debe ser (128,128,3). Es decir, el tamaño de los datos de entrada debe ser el de las imágenes en el dataset.**\n",
        "* **La dimensión de salida debe ser 1. Tan solo necesitamos una neurona para codificar la clase (0 para gatos y 1 para perros) dado que *no* utilizaremos codificación categórica. Notad que la dimensión de salida no se especifica directamente en ningún lugar. Podéis consultar la dimensión de salida utilizando, por ejemplo, el método summary() del modelo.**\n",
        "* **El extractor de características debe estar compuesto por 4 capas (dos pares de Conv2D y MaxPooling2D).**\n",
        "* **Justo después del extractor de características debe haber una capa Flatten().**\n",
        "* **El clasificador debe estar compuesto por dos capas densas. De estas dos capas, la capa oculta debe constar de 128 neuronas.**\n",
        "* **La función de activación de la capa de salida del clasificador (la última capa de la red neuronal) debe ser \"sigmoid\".**\n",
        "* **El número total de parámetros entrenables del modelo debe estar entre los 3 y los 4 millones.**\n",
        "* **Para facilitaros el trabajo ya se proporcionan algunas capas. Por lo tanto, algunos de los requisitos mencionados ya se cumplen.**\n",
        "\n",
        "**Además, vuestro código debe compilar el modelo con el optimizador RMSprop con una tasa de aprendizaje (learning rate) de 0.0001 utilizando \"binary_crossentropy\" como función de pérdida (loss function) y, al menos, *accuracy* como métrica.**\n",
        "\n",
        "**Una vez definido y compilado el modelo, mostrad un resumen del mismo mediante el método summary(). Entre otros, el método summary() mostrará el número de parámetros entrenables que, como se indicó, debe estar entre los 3 y los 4 millones.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cm8EqO8F8Dim"
      },
      "outputs": [],
      "source": [
        "# WRITE YOUR CODE IN THIS CELL AND EXECUTE IT.\n",
        "\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Dense, Flatten\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "# Model creation and compilation is conveniently placed in a function so it is easy to\n",
        "# re-create the model later.\n",
        "def create_and_compile_scratch_model():\n",
        "    theModel=models.Sequential([\n",
        "        Conv2D(32,(3,3),activation='relu',input_shape=(128,128,3)),\n",
        "        MaxPooling2D((2,2),padding='same'),\n",
        "\n",
        "# ---> PUT YOUR CODE TO COMPLETE THE MODEL BETWEEN THIS LINE...\n",
        "\n",
        "\n",
        "# ... AND THIS LINE <---\n",
        "\n",
        "        Dense(1,activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "# ---> PUT YOUR CODE TO COMPILE THE MODEL BETWEEN THIS LINE...\n",
        "\n",
        "\n",
        "# ... AND THIS LINE <---\n",
        "\n",
        "    return theModel\n",
        "\n",
        "theModel=create_and_compile_scratch_model()\n",
        "theModel.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79WllO4O8Dim"
      },
      "source": [
        "**3-b) Entrenad el modelo anterior durante 30 epochs utilizando trainGenerator1 para suminstrar los datos de entrenamiento y valGenerator para suministrar los datos de validación.**\n",
        "\n",
        "**El método de entrenamiento es fit() y devuelve, entre otros, información relativa al histórico del entrenamiento en el campo *.history*. Guardad este histórico en la variable trainHistory.**\n",
        "\n",
        "**Durante el entrenamiento veréis las métricas de calidad que hayáis especificado en el campo \"metrics\" al compilar el modelo. Vuestro objetivo es conseguir una precisión de validación (val\\_accuracy) de, al menos, un 65%. Esto significa que el valor de val\\_accuracy en el último epoch debe ser de, al menos, 0.65.**\n",
        "\n",
        "**Si no conseguís este valor de val\\_accuracy deberéis modificar el modelo en el apartado anterior y repetir el entrenamiento hasta que lo consigáis.**\n",
        "\n",
        "**Si, por algún motivo, paráis el entrenamiento antes de que finalice deberéis crear y compilar el modelo de nuevo antes de volver a entrenar. De no hacerlo el entrenamiento no será correcto.**\n",
        "\n",
        "**En este ejercicio estáis juntando la mayor parte del trabajo de ejercicios anteriores. Por lo tanto, es posible que os aparezcan errores que os obliguen a revisar apartados anteriores.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZVka9868Dim"
      },
      "outputs": [],
      "source": [
        "# WRITE YOUR CODE IN THIS CELL AND EXECUTE IT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hLJezRE8Din"
      },
      "source": [
        "**3-c) Cread las funciones save\\_trained\\_model(fileName,theModel,trainHistory) y [theModel,trainHistory]=load\\_trained\\_model(fileName) encargadas de guardar en disco y cargar desde disco, respectivamente, el modelo entrenado y la historia de su entrenamiento (trainHistory). Con el fin de evitar sobreescrituras accidentales, implementad la función save\\_trained\\_model de forma que si alguno de los archivos a escribir ya existe no haga nada.**\n",
        "\n",
        "**En cuanto a cómo guardar y cargar el modelo entrenado, leed la documentación de Keras. En cuanto a guardar y cargar el trainHistory, nuestra recomendación es utilizar las funciones *dump* y *load* del módulo *pickle*, aunque hay otras opciones igualmente aceptables.**\n",
        "\n",
        "**Tras crear las funciones guardar y cargad el modelo (utilizando \"FIRSTMODEL\" como fileName) para comprobar que todo funciona como se espera**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gl12uNCd8Din"
      },
      "outputs": [],
      "source": [
        "# WRITE YOUR CODE IN THIS CELL AND EXECUTE IT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHqX43I28Din"
      },
      "source": [
        "**3-d) Cread la función plot\\_history(theHistory), donde theHistory es el histórico del entrenamiento del modelo (el trainHistory de celdas anteriores). La función debe crear dos figuras. En la primera debe mostrarse la evolución de la precisión (accuracy) del entrenamiento y de la validación respecto a los epochs. En la segunda se debe mostrar la evolución de la función de pérdida (loss) de entrenamiento y validación respecto a los epochs. Deberéis, por lo tanto, buscar información sobre la estructura del trainHistory.**\n",
        "\n",
        "**Tras crear la función, utilizadla para mostrar las gráficas correspondientes el trainHistory de los apartados anteriores.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVpdGIEc8Din"
      },
      "outputs": [],
      "source": [
        "# WRITE YOUR CODE IN THIS CELL AND EXECUTE IT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaliLFE68Diq"
      },
      "source": [
        "**3-e) Evaluad el modelo (con el método *evaluate()*) utilizando testGenerator. Mostrar la precisión (*accuracy*) resultante.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVI65PwL8Diq"
      },
      "outputs": [],
      "source": [
        "# WRITE YOUR CODE IN THIS CELL AND EXECUTE IT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJi8z1JF8Dir"
      },
      "source": [
        "**3-f) Coged un lote del testGenerator, predecid sus clases mediante el modelo entrnado (utilizando el método *predict()*) y dibujad las imágenes del lote junto con la clase predicha por el modelo. Para ello podéis utilizar la función plot\\_batch programada anteriormente.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNRmSxaX8Dir"
      },
      "outputs": [],
      "source": [
        "# WRITE YOUR CODE IN THIS CELL AND EXECUTE IT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZVioCmQ8Dir"
      },
      "source": [
        "**3-g) Analizad los resultados de 3-d, 3-e y 3-f. ¿Qué ocurre con las precisiones y pérdidas de entrenamiento comparadas con las precisiones y pérdidas de validación? ¿Se ha producido overfitting? ¿Por qué?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHhWM6QU8Dir"
      },
      "source": [
        "**Escribid vuestra respuesta a continuación en esta misma celda.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1ZxqBL58Dir"
      },
      "source": [
        "#### **Ejercicio 4: Mejorando el clasificador**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9Gqbw5s8Dir"
      },
      "source": [
        "En este ejercicio utilizaremos la misma arquitectura que en el ejercicio anterior pero entrenándola con trainGenerator2. Por lo tanto, utilizaremos aumentación de datos. Notad que:\n",
        "\n",
        "* Dado que en este ejercicio básicamente repetiréis las tareas del ejercicio 3 (pero utilizando un generador de datos de entrenamiento distinto) podéis llamar a cualquier función que hayáis creado en el ejercicio anterior.\n",
        "* El tiempo de entrenamiento debería ser similar al del ejercicio 3 (dado que el modelo es el mismo). Si observáis que es mucho mayor deberíais optimizar la parte de aumentación de datos del DataGenerator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmZpHwMV8Dir"
      },
      "source": [
        "**4-a) Cread y compilad de nuevo el modelo anterior (para reiniciarlo) llamando a la función create\\_and\\_compile\\_scratch\\_model(). Después, entrenadlo con trainGenerator2 para los datos de entrenamiento y valGenerator para los datos de validación durante 30 epochs. Guardad el histórico de entrenamiento en trainHistory y guardar el modelo con save\\_trained\\_model (utilizando SECONDMODEL como fileName).**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqQLkKhg8Dir"
      },
      "outputs": [],
      "source": [
        "# WRITE YOUR CODE IN THIS CELL AND EXECUTE IT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDQ13g3c8Dir"
      },
      "source": [
        "**4-b) Dibujar el histórico del entrenamiento (mediante la función plot\\_history creada anteriormente). Tras esto, evaluad el modelo con testGenerator e imprimid la precisión (accuracy) obtenida. Finalmente, coged un lote de testGenerator (con \\_\\_getitem\\_\\_), predecidlo con este nuevo modelo y dibujad los resultados con plot\\_batch.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2HbtvF08Dir"
      },
      "outputs": [],
      "source": [
        "# WRITE YOUR CODE IN THIS CELL AND EXECUTE IT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7jQlBbG8Dis"
      },
      "source": [
        "**4-c) Analizad los resultados en 4-b y comparadlos con los del ejercicio 3. ¿Hay overfitting ahora? ¿Por qué? ¿Creéis que el modelo podría entrenarse durante más epochs?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCqVW5ZN8Dis"
      },
      "source": [
        "**Mostrad vuestra respuesta a continuación en esta misma celda.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRQHoBks8Dis"
      },
      "source": [
        "#### **Ejercicio 5: Transfer learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_J3ycCy8Dis"
      },
      "source": [
        "Llegados a este punto deberíais haber detectado dos de las debilidades de las redes neuronales: la necesidad de grandes cantidades de datos para su entrenamiento (miles de imágenes no han sido suficientes incluso con aumentación de datos) y los largos tiempos de entrenamiento. Por lo tanto, lo único que nos puede ayudar para entrenar adecuadamente una red neuronal es un conjunto de datos extremadamente grande y un ordenador extremadamente potente.\n",
        "\n",
        "Bueno... en realidad no necesariamente. Si alguien ha entrenado previamente una red neuronal con cantidades ingentes de imágenes similares a las nuestras durante un tiempo muy grande y ha hecho público el modelo entrenado, podremos utilizarlo y adaptarlo a nuestro problema concreto. Este proceso se conoce como *transfer learning*.\n",
        "\n",
        "En este ejercicio exploraremos el *transfer learning* mediante la arquitectura Xception previamente entrenada con el dataset Imagenet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLAj404q8Dis"
      },
      "source": [
        "**5-a) Buscad información y describid brevemente la arquitectura Xception y el dataset Imagenet.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AeB7fHi8Dis"
      },
      "source": [
        "**Mostrad vuestra respuesta a continuación en esta misma celda.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C-hONZ08Dis"
      },
      "source": [
        "**5-b) Cread una instancia de la arquitectura Xception especificando que la dimensión de entrada (input\\_shape) es (128,128,3), que la queremos pre-entrenada con Imagenet y que no queremos instanciar el clasificador (tan solo el extractor de características). Después, haced que esta instancia sea *NO* entrenable.**\n",
        "\n",
        "**Notad que:**\n",
        "* **La instancia *NO* debe ser entrenable dado que utilizaremos el extractor de características tal y como es y sólo entrenaremos nuestro clasificador.**\n",
        "* **Para instancias Xception, buscad Xception en la documentación del paquete keras.applications.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MySF5ITw8Dis"
      },
      "outputs": [],
      "source": [
        "# WRITE YOUR CODE IN THIS CELL AND EXECUTE IT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soIV3F9z8Dis"
      },
      "source": [
        "**5-c) Completad la función create\\_and\\_compile\\_Xception\\_model proporcionada en la siguiente celda de forma que la instancia de Xception creada en la actividad anterior se use como extractor de características. El clasificador debe tener la misma estructura que en las actividades anteriores. La función de pérdida, las métricas y el optimizador también deben ser los mismos que en apartados anteriores. Tras hacerlo, ejecutad la celda, examinad el resumen (summary()) del modelo y responded a las siguientes preguntas.**\n",
        "\n",
        "* **¿Cuántos parámetros entrenables y no entrenables tiene el modelo?**\n",
        "* **¿Por qué este modelo tiene tantos parámetros no entrenables?**\n",
        "* **¿Qué propósito tiene tener tantos parámetros no entrenables?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJb81NEj8Dit"
      },
      "outputs": [],
      "source": [
        "# WRITE YOUR CODE IN THIS CELL AND EXECUTE IT.\n",
        "\n",
        "def create_and_compile_Xception_model():\n",
        "    theModel=models.Sequential([\n",
        "\n",
        "# ---> PUT YOUR CODE TO COMPLETE THE MODEL BETWEEN THIS LINE...\n",
        "\n",
        "\n",
        "# ... AND THIS LINE <---\n",
        "\n",
        "        Dense(1,activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "# ---> PUT YOUR CODE TO COMPLETE THE MODEL BETWEEN THIS LINE...\n",
        "\n",
        "\n",
        "# ... AND THIS LINE <---\n",
        "    return theModel\n",
        "\n",
        "theModel=create_and_compile_Xception_model()\n",
        "theModel.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbJyiH_v8Dit"
      },
      "source": [
        "**Responded a continuación en esta misma celda a las preguntas formuladas**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTvLsttD8Dit"
      },
      "source": [
        "**5-d) Entrenad el modelo anterior durante 30 epochs utilizando trainGenerator2 como proveedor de datos de entrenamiento y valGenerator para proporcionar datos de validación. Guardad el histórico del entrenamiento y guardadlo en disco junto con el modelo entrenado mediante save\\_trained\\_model utilizando XCEPTIONMODEL como fileName.**\n",
        "\n",
        "**El tiempo de entrenamiento en este caso puede ser superior al de los modelos previos, aunque no demasiado. Podéis esperar un incremento cercano al 50% en tiempo.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2am1Mix8Dit"
      },
      "outputs": [],
      "source": [
        "# WRITE YOUR CODE IN THIS CELL AND EXECUTE IT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJOKoL_U8Dit"
      },
      "source": [
        "**5-e) Utilizando las funciones creadas en ejercicios anteriores, se pide:**\n",
        "\n",
        "* **Dibujad los históricos de entrenamiento.**\n",
        "* **Evaluad el modelo con testGenerator e imprimid la precisión resultante (accuracy)**\n",
        "* **Predecid un lote del testGenerator y dibujad las imágenes que contiene junto con la clase predicha (CAT o DOG).**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v37mkiXr8Dit"
      },
      "outputs": [],
      "source": [
        "# WRITE YOUR CODE IN THIS CELL AND EXECUTE IT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQqGuNSW8Dit"
      },
      "source": [
        "**5-f) Analizad los resultados obtenidos. ¿Hay overfitting? ¿Son los resultados mejores que en ejercicios anteriores? ¿Por qué?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMwZYQ7a8Dit"
      },
      "source": [
        "**Escribid vuestra respuesta a continuación en esta misma celda**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wruFWOB8Dit"
      },
      "source": [
        "#### ***Ejercicio 6: Más allá de los clasificadores***\n",
        "\n",
        "En este ejercicio no debéis programar. Debéis buscar información y reflexionar (partiendo de lo que habéis aprendido en los ejercicios anteriores) acerca de las preguntas que se formulan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivYojMQq8Dit"
      },
      "source": [
        "**6-a) ¿Qué creéis que ocurriría si intentáramos clasificar, con alguno de los modelos anteriores, una imagen que no contenga ni un perro ni un gato? ¿Qué haríais para extender el modelo de forma que clasificara gatos, perros y caballos?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wre4cZUb8Diu"
      },
      "source": [
        "**Mostrad vuestra respuesta a continuación en esta misma celda**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf8GIpEz8Diu"
      },
      "source": [
        "**6-b) Las redes neuronales pueden utilizarse para identificar caras humanas. Es decir, para decidir quien es la persona cuya cara se muestra en una imagen. Esto es también un problema de clasificación y se utiliza en sistema de seguridad o, recientemente, para permitir el acceso a *smartphones*.**\n",
        "\n",
        "**¿Creéis que estos sistemas funcionan de forma parecida a los que habéis implementado en esta práctica? Buscad información acerca de redes neuronales para reconocimiento facial y reflexionad sobre si sus arquitecturas son similares o no a las de esta práctica.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYTMPz208Diu"
      },
      "source": [
        "**Mostrad vuestra respuesta a continuación en esta misma celda.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlnkRb4CiDsR"
      },
      "source": [
        "### Nota: Propiedad intelectual\n",
        "\n",
        "A menudo es inevitable, al producir una obra multimedia, hacer uso de recursos creados por terceras personas. Es por tanto comprensible hacerlo en el marco de una práctica de los estudios del Grado Multimedia, siempre y cuando esto se documente claramente y no suponga plagio en la práctica.\n",
        "\n",
        "Por lo tanto, al presentar una práctica que haga uso de recursos ajenos, se presentará junto con ella un documento en el que se detallen todos ellos, especificando el nombre de cada recurso, su autor, el lugar donde se obtuvo y el su estatus legal: si la obra está protegida por copyright o se acoge a alguna otra licencia de uso (Creative Commons, GNU, GPL ...). El estudiante deberá asegurarse de que la licencia que sea no impide específicamente su uso en el marco de la práctica. En caso de no encontrar la información correspondiente deberá asumir que la obra está protegida por copyright.\n",
        "\n",
        "Deberán, además, adjuntar los archivos originales cuando las obras utilizadas sean digitales, y su código fuente si corresponde.\n",
        "\n",
        "Otro punto a considerar es que cualquier práctica que haga uso de recursos protegidos por copyright no podrá en ningún caso publicarse en Mosaic, la revista del Grado en Multimedia en la UOC, a no ser que los propietarios de los derechos intelectuales den su autorización explícita.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}